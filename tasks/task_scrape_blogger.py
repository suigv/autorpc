# tasks/task_scrape_blogger.py
import time
import re
from common.bot_agent import BotAgent
from common.x_scheme import XScheme
from common.blogger_manager import BloggerManager

def scrape_bloggers(bot, keyword):
    """
    æ‰§è¡Œé‡‡é›†åŠ¨ä½œ (æ»‘åŠ¨å¾ªç¯ + æ™ºèƒ½æ’é™¤)
    :return: list of bloggers
    """
    bot.log(f"ğŸ” å¼€å§‹é‡‡é›†åšä¸»ï¼Œå…³é”®è¯: {keyword}")
    
    # 1. æœç´¢
    search_uri = XScheme.get_url(XScheme.SEARCH, query=keyword, latest=True)
    bot.shell_cmd(XScheme.wrap_command(search_uri))
    time.sleep(8) # ç­‰å¾…åŠ è½½
    
    # 2. å¼ºåˆ¶åˆ‡æ¢åˆ° "æœ€æ–°" (Live)
    if bot.exists_desc("æœ€æ–°"):
        bot.log("ğŸ‘‰ åˆ‡æ¢åˆ° [æœ€æ–°] æ ‡ç­¾")
        bot.click_desc("æœ€æ–°")
        time.sleep(4)
    
    collected = []
    max_swipes = 5
    swipe_cnt = 0
    
    # æ ‡ç­¾å…³é”®è¯ (å»é™¤ #)
    tag_key = keyword.replace("#", "")
    
    while len(collected) < 10 and swipe_cnt < max_swipes:
        selector = bot.rpa.create_selector()
        if selector:
            with selector:
                selector.addQuery_IdEqual("com.twitter.android:id/row")
                nodes = selector.execQuery(20, 3000)
                
                if nodes:
                    for n in nodes:
                        desc = n.get_node_desc()
                        if not desc: continue
                        
                        # æå–æ‰€æœ‰ @username
                        matches = re.findall(r"@([a-zA-Z0-9_]+)", desc)
                        
                        if not matches: continue
                        
                        # æ™ºèƒ½æ’é™¤é€»è¾‘
                        sender = matches[0]
                        candidates = [m for m in matches[1:] if m != sender]
                        
                        if tag_key in desc:
                            parts = desc.split(tag_key)
                            if len(parts) > 1:
                                after_tag = parts[1]
                                tag_matches = re.findall(r"@([a-zA-Z0-9_]+)", after_tag)
                                tag_candidates = [m for m in tag_matches if m != sender]
                                if tag_candidates:
                                    candidates = tag_candidates
                        
                        for t in candidates:
                            if t not in collected:
                                collected.append(t)
                                bot.log(f"â• æ•è·åšä¸»: {t}")
        
        if len(collected) >= 10:
            break
            
        bot.swipe_screen("up", distance=0.6)
        swipe_cnt += 1
        time.sleep(3)
    
    return collected

def ensure_blogger_ready(device_info, ai_type):
    """
    ç¡®ä¿æœ‰å¯ç”¨åšä¸» (ä¾›å…¶ä»–ä»»åŠ¡è°ƒç”¨)
    :return: (blogger, is_new_binding)
    """
    idx = device_info['index']
    ip = device_info['ip']
    
    # 1. å°è¯•è·å–
    blogger, need_scrape, is_new = BloggerManager.get_blogger(idx, ai_type)
    
    if blogger:
        return blogger, is_new
        
    if need_scrape:
        # 2. æ‰§è¡Œé‡‡é›†
        bot = BotAgent(idx, ip)
        if bot.connect():
            keyword = "#mytxx" if ai_type == "volc" else "#mytjz"
            new_bloggers = scrape_bloggers(bot, keyword)
            count = BloggerManager.add_bloggers(ai_type, new_bloggers)
            bot.log(f"âœ… é‡‡é›†å®Œæˆï¼Œå…¥åº“ {count} ä¸ª")

            # ä»…åœ¨å®é™…é‡‡é›†åˆ°åšä¸»æ—¶è®°å½•é‡‡é›†æ—¶é—´ã€‚
            # è‹¥ count == 0ï¼Œä¸å†™å…¥å†·å´çŠ¶æ€ï¼Œä¿è¯åç»­ä»ä¼˜å…ˆè§¦å‘é‡‡é›†ã€‚
            if count > 0:
                BloggerManager.update_scrape_time(idx, ai_type)
            else:
                bot.log("âš ï¸ æœ¬æ¬¡é‡‡é›†ä¸º0ï¼Œæœªå†™å…¥å†·å´çŠ¶æ€ï¼Œåç»­å°†ç»§ç»­ä¼˜å…ˆé‡‡é›†")

            bot.quit()
            
            # 3. å†æ¬¡å°è¯•è·å–
            blogger, _, is_new = BloggerManager.get_blogger(idx, ai_type)
            return blogger, is_new
            
    return None, False
